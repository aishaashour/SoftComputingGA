package org.neuralNetwork;
import java.util.ArrayList;
import java.util.List;

import org.neuralNetwork.Layer.ILayer;
import org.neuralNetwork.Loss.ILoss;

public class NerualNetwork {
    private List<ILayer> layers;
    private ILoss lossFunction;

    public NerualNetwork() {
        this.layers = new ArrayList<>();
    }

    public void addLayer(ILayer layer) {
        this.layers.add(layer);
    }

    public void setLossFunction(ILoss lossFunction) {
        this.lossFunction = lossFunction;
    }
    /**
     * Forward Propagation:
     * Passes the input through all layers sequentially.
     * @return The final output of the network.
     */
    public double[][] predict(double[][] input) {
        double[][] output = input;
        // Forward pass through each layer
        for (ILayer layer : layers) {
            output = layer.forward(output);
        }
        return output;
    }

    /**
     * The Main Training Loop (fit).
     * Handles batching, forward pass, loss calculation, and backward pass.
     */
    // Each epoch = one full pass over the dataset.
    public void train(double[][]xTrain, double[][] yTrain, int epochs, int batchSize, double learningRate) {
        
        int sampleSize = xTrain.length;
        for (int epoch = 0; epoch < epochs; epoch++) {
            double totalEpochLoss = 0.0;
            int batches=0;
            for (int start=0; start< sampleSize; start+=batchSize){
                int end = Math.min(start + batchSize, sampleSize);

                //1.Miini-batchs creation
                double[][] xBatch = slice(xTrain, start, end);
                double[][] yBatch = slice(yTrain, start, end);
               
                // 2. Forward Pass
                double[][] predictions = predict(xBatch);

                // 3. Loss Calculation
                totalEpochLoss += lossFunction.computeLoss(predictions, yBatch);
                batches++;

                // 4. Backward Pass
                // Calculate initial gradient (dE/dY) using the loss function
                double[][] gradient = lossFunction.computeGradient(predictions, yBatch);

                // Propagate gradient backwards through layers in reverse order
                for (int i = layers.size() - 1; i >= 0; i--) {
                    ILayer layer = layers.get(i);
                    // The layer updates its own weights internally and returns gradient for the next layer
                    gradient = layer.backward(gradient, learningRate);
                }
                
                // Print average loss every 10 epochs (or every epoch)
                if ((epoch + 1) % 10 == 0 || epoch==0) {
                    double averageLoss = totalEpochLoss / batches;
                    System.out.println("Epoch " + (epoch + 1) + ", Batch " + (batches) + ", Loss: " + averageLoss);
                }
            }   

        }
    }
    private double[][] slice(double[][] data, int start, int end) {
        int length = end - start;
        int features = data[0].length;
        double[][] batch = new double[length][features];
        
        for (int i = 0; i < length; i++) {
            // Copying reference is faster and usually safe for read-only training data
            batch[i] = data[start + i]; 
        }
        return batch;
    }
}